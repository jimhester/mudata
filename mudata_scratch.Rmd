---
title: "MUData"
author: "Dewey Dunnington"
date: "11/18/2016"
output: github_document
---

## Introduction

All data storage is a balance between flexibility (what information can it store), efficiency (how much disk space is needed to store the data), and usability (how much manipulation is needed before something can be done with it). 

## Schema

Data should be stored in 3 tables: `data`, `locations`, and `params`. This assumes there is some spatial significance to the data, and that the data has some number of `params` that were measured for each `locations` along an axis `x` (time, depth, or could even be non-existent). This data has a `val`, an `err`, and some arbitrary number of `tags` associated with it. The storage of this data may seem complex, but has the necessary flexibility to contain most datasets.

The storage of this data is suited to a database. Databases are not directly openable by some programs, but allow greater flexibility, and data can easily be extracted in plottable form using common database and reshaping operations. This allows the framework to be language-agnostic, allowing for implementation in multiple languages. The database schema is as follows:

```{r results='asis'}
knitr::kable(data.frame(Table=c("data", "locations", "params", "datasets"),
                        Columns=c("dataset, location, x, param, value, tags", 
                                  "dataset, location, lon, lat, tags",
                                  "dataset, param, tags",
                                  "dataset, tags")))
```

Columns are limited to things that are queried. While the `tags` structure is flexible, standardization is essential to make the database useful. Therefore, essential components such as `lon`, `lat`, and `value` are placed in columns instead of in `tags`. The `data` table is constructed such that no other information is necessary to plot its contents. This means that human-readable IDs should be used for `location` and `param`, instead of traditional integer-based identifiers. This makes the format less efficient for storage but more usable.

Non-detect values can be stored as `NULL` (or `NA`) in the table, whereas values that were not measured should not have a row in the `data` table.

The combination of dataset/location/x/param must form a unique identifier for each value.

Tags can have an arbitrary key/value format, although JSON is recommended. Tags should not have different meanings for a key/value pair missing and a key/value pair as `NA` (this leads to data loss when expanding out tags into columns).

## Example in R

The following is data collected from Alta Lake in 2014 (Dunnington 2015; Dunnington et al. 2016).

```{r}
sample_data <- read.csv('data/sample_data.csv', stringsAsFactors = F)
sample_locations <- read.csv('data/sample_locations.csv', stringsAsFactors = F)
sample_params <- read.csv('data/sample_params.csv', stringsAsFactors = F)
sample_datasets <- read.csv('data/sample_datasets.csv', stringsAsFactors = F)

knitr::kable(head(sample_data))
knitr::kable(sample_locations)
knitr::kable(sample_params)
knitr::kable(sample_datasets)

sample_mudata <- structure(.Data=list(data=as.qtag(sample_data, 
                      .qualifiers = c('dataset', 'location', 'x', 'param'),
                      .values='value',
                      .tags='tags'), 
                           locations=sample_locations, 
                           params=sample_params,
                           datasets=sample_datasets), class=c('mudata', 'list'))

autoplot(sample_mudata$data)
```

Expanding tags to columns

```{r}
library(dplyr)
sample_data_expanded <- sample_data %>% group_by(dataset, location, x, param) %>%
  do({
    cbind(., as.data.frame(jsonlite::fromJSON(as.character(.$tags))))
  })
sample_data_expanded$tags <- NULL
knitr::kable(head(sample_data_expanded))
```

Plotting

```{r}
library(ggplot2)
ggplot(sample_data, aes(x=value, y=x)) + geom_path() + 
  scale_y_reverse() +
  facet_wrap(~param, scales="free_x")

library(ofcores)
sdqtag <- as.qtag(sample_data, .qualifiers = c('dataset', 'location', 'x', 'param'), 
                  .values = 'value')
sample_data_wide <- wide(sdqtag)
```

### Getting data into the schema

```{r, eval=FALSE}
library(ofcores)
dx <- read.csv('data/ALGC14-1_wide.csv')[c(-2, -3)]
dxnames <- names(dx)
qs <- 'depth'
stdev <- dxnames[grepl('stdev_', dxnames)]
n <- dxnames[grepl('n_', dxnames)]
vals <- dxnames[!(dxnames %in% c(qs, stdev, n))]

dxmelt <- melt.parallel(dx, qs, variable.name = 'param', value=vals, stdev=stdev, n=n)
dxmelt <- dxmelt[!(dxmelt$n == 0 & dxmelt$param == "SAR"),]
dxmelt$stderr <- dxmelt$stdev/sqrt(dxmelt$n)

dxmelt$location <- "ALGC14-1"
dxmelt <- dxmelt[c('location', 'depth', 'param', 'value', 'stdev', 'n', 'stderr')]

dxmelt$tags <- sapply(1:nrow(dxmelt), function(i) {
  vals <- sapply(c('stdev', 'n', 'stderr'), function(name) {
    dxmelt[[name]][i]
    })
  vals <- vals[!is.na(vals)]
  paste0('{', paste0('"', names(vals), '": ', vals, collapse=", "), '}')
})
dxmelt$dataset <- 'Dunn2016'
dxmelt <- plyr::rename(dxmelt, c('depth'='x'))
write.csv(dxmelt[c('dataset', 'location', 'x', 'param', 'value', 'tags')], 'data/sample_data.csv', row.names = F)

# locations
locations <- data.frame(dataset='Dunn2016', location='ALGC14-1', lon=-122.293, lat=44.193, tags='{}')
write.csv(locations, 'data/sample_locations.csv', row.names=F)

# params
params <- data.frame(dataset='Dunn2016', param=c("x", vals), tags='{"unit": "ppm"}', 
                     stringsAsFactors = FALSE)
params$tags[params$param %in% c("Fe.Mn", "C.N")] <- '{}'
params$tags[params$param %in% c("d13C", "d15N")] <- '{"unit": "permille"}'
params$tags[params$param %in% c("C", "K", "Ti")] <- '{"unit": "%"}'
params$tags[params$param == "SAR"] <- '{"unit": "g/m2/yr"}'
params$tags[params$param == "x"] <- '{"unit": "cm", "type":"depth"}'

write.csv(params, 'data/sample_params.csv', row.names = F)

# datasets
datasets <- data.frame(dataset='Dunn2016', tags='{}')
write.csv(datasets, 'data/sample_datasets.csv', row.names=F)

```

